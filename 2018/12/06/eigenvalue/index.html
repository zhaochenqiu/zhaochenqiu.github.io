<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>奇异值分解及其应用 | Chenqiu&#39;s Blog</title>
  <meta name="author" content="Zhao, Chenqiu">
  
  <meta name="description" content="详述奇异值分解与PCA，最小二乘，数据拟合，3D转换的关系">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="奇异值分解及其应用"/>
  <meta property="og:site_name" content="Chenqiu&#39;s Blog"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Chenqiu&#39;s Blog" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  

</head>


<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">Chenqiu&#39;s Blog</a></h1>
  <h2><a href="/">Do What You Like, Like What You Do</a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2018-12-07T04:08:00.000Z"><a href="/2018/12/06/eigenvalue/">2018-12-06</a></time>
      
      
  
    <h1 class="title">奇异值分解及其应用</h1>
  

    </header>
    <div class="entry">
      
        <p>详述奇异值分解与PCA，最小二乘，数据拟合，3D转换的关系
<a id="more"></a></p>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<h1 id="奇异值分解"><a href="#奇异值分解" class="headerlink" title="奇异值分解"></a>奇异值分解</h1><p>奇异值分解的目的是将一个\(m \times n \)的矩阵\( A \)分解成另外三个矩阵\(U, \Sigma, V^T \).其中\(U\)和\(V\)分别为\(m \times m\)和\(n \times n\)的方阵,
\(\Sigma\)为\(m \times n\)的矩阵.数学表达形式如下:
\[
\large
A_{m \times n} = U_{m \times m} \Sigma_{m \times n} V^T_{n \times n}
\]</p>
<h2 id="特征值与特征值分解"><a href="#特征值与特征值分解" class="headerlink" title="特征值与特征值分解"></a>特征值与特征值分解</h2><p>谈奇异值之前必须要介绍特征值分解，因为奇异值分解实际是特征值分解的一个扩展.
特征值分解是将一个矩阵分解成他的特征值以及特征向量,先来看特征值与特征向量的定义:
\[
\large
A \vec{v} = \lambda \vec{v}
\]
如上式，特征向量就是指矩阵与这个向量运算后得到的向量，只有长度的变化，没有方向的变化。更甚着，这种运算可以理解为矩阵在这个向量上的投影（之后会解释为什么可以理解为投影）。
而且，矩阵的特征向量是不唯一的。如果将矩阵所有特征向量和特征值综合到一起，就可以得到矩阵的特征值分解公式，具体过程如下：
\[
\begin{split}
&amp; A \vec{v}_1 = \lambda_1 \vec{v}_1 \\
&amp; A \vec{v}_2 = \lambda_2 \vec{v}_2 \\
&amp; \cdots \\
&amp; A \vec{v}_m = \lambda_m \vec{v}_m \\
\\
 \Rightarrow  &amp; A[\vec{v}_1 \ \vec{v}_2 \ \cdots \vec{v}_m] = [\lambda_1\vec{v}_1 \ \lambda_2\vec{v}_2 \ \cdots \lambda_m\vec{v}_m] =  \begin{bmatrix}
\lambda_1 v_{1,1} &amp; \lambda_2 v_{1,2} &amp; \cdots &amp; \lambda_m v_{1,m} \\
\lambda_1 v_{2,1} &amp; \lambda_2 v_{2,2} &amp; \cdots &amp; \lambda_m v_{2,m} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\lambda_1 v_{m,1} &amp; \lambda_2 v_{m,2} &amp; \cdots &amp; \lambda_m v_{m,m}
\end{bmatrix} \\
&amp; = \begin{bmatrix}
v_{1,1} &amp; v_{1,2} &amp; \cdots &amp; v_{1,m} \\
v_{2,1} &amp; v_{2,2} &amp; \cdots &amp; v_{2,m} \\
\vdots &amp; \vdots &amp; \ddots &amp; \\
v_{m,1} &amp; v_{m,2} &amp; \cdots &amp; v_{m,m}
\end{bmatrix} \begin{bmatrix}
\lambda_1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \lambda_2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \lambda_m
\end{bmatrix} \\
&amp; = [\vec{v}_1 \ \vec{v}_2 \cdots \vec{v}_m] \begin{bmatrix}
\lambda_1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \lambda_2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \lambda_m
\end{bmatrix} \\
\Rightarrow &amp; AV = V \Lambda, \quad V = [\vec{v}_1 \ \vec{v}_2 \cdots \vec{v}_m] \quad \Lambda = \begin{bmatrix}
\lambda_1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \lambda_2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \lambda_m
\end{bmatrix}  \\
\Rightarrow &amp; A = V \Lambda V^{-1} \\
\end{split} \\
\]
到这一步已经得到的特征值分解的如下定义：
\[
A = V \Lambda V^{-1}, \quad V = [\vec{v}_1 \ \vec{v}_2 \cdots \vec{v}_m] \quad \Lambda = \begin{bmatrix}
\lambda_1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \lambda_2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \lambda_m
\end{bmatrix} 
\]
更甚者，<strong>当\(A\)是一个实对阵矩阵的时，他不同特征值对应的特征向量两两正交</strong>，也就是说上式的\(V\)是一个正交矩阵而且他满秩，
即\(V^{-1} = V^T \).所以上式又可以写成：
\[
\large
A = V \Lambda V^{-1} = V \Lambda V^T
\]</p>
<p>所以这里，我们就得到了一个线性代数中非常著名的定理.</p>
<p><strong>当\(A \)是一个实对称矩阵时，一定可以找到一个由他特征向量组成的矩阵\(V\)，和一个由对应特征值组成的对角矩阵\(\Lambda\)，构成如下形式：</strong>
\[
A = V \Lambda V^{-1} = V \Lambda V^T
\]</p>
<p>证明上述定理的核心，就是为什么当\(A\)是实对称矩阵时，\(V^{-1} = V^T\).
为此，需要引入很多其他的证明:</p>
<h4 id="1-A为实对称矩阵时候，一定有n个实数特征值，而且不同特征值对应的特征向量一定正交"><a href="#1-A为实对称矩阵时候，一定有n个实数特征值，而且不同特征值对应的特征向量一定正交" class="headerlink" title="1). A为实对称矩阵时候，一定有n个实数特征值，而且不同特征值对应的特征向量一定正交."></a>1). A为实对称矩阵时候，一定有n个实数特征值，而且<strong>不同特征值对应的特征向量一定正交.</strong></h4><p>首先有实数特征值很容易理解，特征值的计算方法是\( |A - \lambda I| = 0\).
该式是一个\(n \)阶的多项式,所以一定有\(n\)个实数根.也就是说有\(n\)个实数的特征值.</p>
<p>当特征值不相等时,即当\(\lambda_1 \neq \lambda\),有如下推论:
\[
\begin{split}
 &amp; A \vec{v}_i = \lambda_i \vec{v}_i \\
 &amp; A  \vec{v}_j = \lambda_j \vec{v}_j \\ \\
\Rightarrow &amp; x_j^T A x_i = x_j^T \lambda_i x_i = \lambda_i x_j^T x_i \\
\Rightarrow &amp; \lambda_i x_j^T x_i = x_j^T \lambda_i x_i = x_j^T A x_i = x_j^T A^T x_i \quad :A = A^T \\
\Rightarrow &amp; \lambda_i x_j^T x_i =  x_j^T A^T x_i = (A x_j)^T x_i = \lambda_j x_j^T x_i \\
\Rightarrow &amp; \lambda_i x_j^T x_i =  \lambda_j x_j^T x_i \\
\Rightarrow &amp; (\lambda_i - \lambda_j)x_j^T x_i \\ \\
\because &amp; \lambda_i \neq \lambda_j  \therefore x_j^Tx_i = 0 \\
\Rightarrow &amp; x_j \perp x_i
\end{split}
\]
这里得到结论：<strong>当特征值不同时，对应的特征向量一定相互正交，也就是相互垂直.</strong></p>
<h4 id="2-当特征值相同时，特征向量一定相同-因为："><a href="#2-当特征值相同时，特征向量一定相同-因为：" class="headerlink" title="2). 当特征值相同时，特征向量一定相同,因为："></a>2). 当特征值相同时，特征向量一定相同,因为：</h4><p>\[
\begin{split}
&amp; A \vec{v} = \lambda \vec{v} \\
\Rightarrow &amp; A\vec{v} - \lambda \vec{v} = 0 \\
\Rightarrow &amp; (A - \lambda I) \vec{v} = 0
\end{split}
\]
是一个方程组。而且由于\(A\)是一个\(n\)阶的方阵，所以当\( \lambda \)唯一时，方程组的解一定唯一。
也就是说，对于一个方阵，<strong>当特征值确定时，特征向量一定唯一。</strong></p>
<p>再回到对称矩阵\(A\)。这里要分成两步讨论:</p>
<p>1) 当这个矩阵的所有的特征值都不同时，对于的特征向量一定正交。
更甚者，可以将特征向量归一化到模长为1，多余的常数可以存储到特征值中。
所以这个时候\(V\)就是一个正交矩阵，而且每一列的向量模长都为1.
所以这个时候，\(V \times V^T  = I\),也就是说\(V^{-1} = V^T \).</p>
<p>2) 当这个矩阵的特征值出现重根的时候。对应的A矩阵一定不满秩。
因为特征值相同，也就反推出特征向量相同。而对于公式\(A = V \Lambda V^{-1} \)对于所有的方阵都实用，
所以重构出来的矩阵A，一定会有4个元素相等，如下:
\[
\begin{bmatrix}
\ddots &amp; \vdots &amp;  &amp; \vdots &amp;  \\
\cdots &amp; \lambda_i \vec{v}_i \vec{v}_i^T &amp; \cdots &amp; \lambda_i \vec{v}_i \vec{v}_j^T &amp; \cdots \\ 
 &amp; \vdots &amp;  &amp; \vdots &amp;  \\ 
\cdots &amp; \lambda_j \vec{v}_i \vec{v}_j^T &amp; \cdots &amp; \lambda_j \vec{v}_j \vec{v}_j^T &amp; \cdots \\
  &amp; \vdots &amp;  &amp; \vdots &amp; \ddots
\end{bmatrix}
\]
也就是推出有两个行向量是线性相关的。所以他就不满秩。
而对于不满秩的部分，可以用0特征值来补全\(\Lambda\)矩阵，对应的\(V\)中的就用一个很其他所有特征向量的正交的向量。
由于\(A\)是方阵，所以它是一个\(R^m\)的空间，肯定存在\(m\)个相互正交的向量。
因此一定找得到足够数量的相互正交的向量来填充\(V\)矩阵，使得V变成一个相互正交的矩阵，也就是\(V^{-1} = V^T\).</p>
<p>综上，<strong>只要\(A\)是一个实对称矩阵（不一定需要满秩）,一定找得到合适的\(V\)和\(\Lambda\)
使得\(A = V \Lambda V^T \)满足。而且非0不重复特征值数量一定是\(A\)的秩的个数.</strong></p>
<h2 id="奇异值分解与特征值分解"><a href="#奇异值分解与特征值分解" class="headerlink" title="奇异值分解与特征值分解"></a>奇异值分解与特征值分解</h2><p>已经证明出来，对于一个任意对角方阵，一定可以找出合适的\(V\)和\(Lambda\)使得下式成立：
\[
A = V \Lambda V^T
\]
那么对已一个任意的矩阵，能不能将其拆分成该形式呢？答案是肯定的，奇异值分解就是为了达到这一目的.</p>
<p>现在假设有任意矩阵\(A \)，\(A^T A \)一定是一个实对称矩阵。
因此，对于\(A^T A\)，我们可以进行特征值分解，得到结果如下：
\[
A^T A = V \Lambda V^T
\]
得到上述结果后\(V\)的行向量可以拆分成一组正交基\(\{ \vec{v}_1, \vec{v}_2 \cdots \vec{v}_n \}\)而且这组正交基一定有如下性质：
\[
(A^T A) \vec{v}_i = \lambda_i \vec{v}_i
\]
然后我们来验证下\(A \vec{v}_i \), 和\(A \vec{v}_j \)的正交性：
\[
\begin{split}
(A \vec{v}_i, A \vec{v}_j) &amp; = (A \vec{v}_i)^T A \vec{v}_j \\
&amp; = \vec{v}_i^T A^T A \vec{v}_j \\
&amp; = \vec{v}_i^T (A^T A \vec{v}_j) \\
&amp; = \vec{v}_i^T ( \lambda_j \vec{v}_j ) \\
&amp; = \lambda_j \vec{v}_i^T \vec{v}_j \\
&amp; = 0
\end{split}
\]
这样，我们又得到了另一组正交基\(\{ A\vec{v}_1, A\vec{v}_2 \cdots A\vec{v}_n \}\).接着我们将这组正交基标准化：
\[
\begin{split}
&amp; \vec{u}_i = \frac{A \vec{v}_i}{|A \vec{v}_i|} = \frac{1}{\lambda_i} A \vec{v}_i \\
\Rightarrow &amp; A \vec{v}_i = \sqrt{\lambda_i} \vec{u}_i \\
&amp; \because |A \vec{v}_i|^2 = \lambda_i \vec{v}_i^T \vec{v}_i = \lambda_i \\
&amp; \therefore |A \vec{v}_i| = \sqrt{\lambda_i} = \delta_i \\
\Rightarrow &amp; A \vec{v}_i = \delta_i \vec{u}_i
\end{split}
\]
接着继续推导：
\[
\begin{split}
A V &amp;  = A(\vec{v}_1, \vec{v}_2, \cdots, \vec{v}_r) = (A\vec{v}_1, A\vec{v}_2, \cdots, A\vec{v}_r, 0, \cdots, 0) \\
&amp; = (\delta_1 \vec{u}_1, \delta_2 \vec{u}_2, \cdots, \delta_r \vec{u}_r, 0, \cdots, 0 ) = U \Sigma \\
\Rightarrow &amp; A = U \Sigma V^T
\end{split}
\]
如此，我们就得到的奇异值分解的公式：
\[
A = U \Sigma V^T
\]
其中，右奇异向量\(V\)为\(A^T A\)的特征向量。
奇异值\(\Sigma\)为\(A^T A\)的特征值开根号。
左奇异向量为\( \frac{1}{\sqrt{\lambda_i}}A \vec{v}_i \).</p>
<h2 id="奇异值特征值分解的意义"><a href="#奇异值特征值分解的意义" class="headerlink" title="奇异值特征值分解的意义"></a>奇异值特征值分解的意义</h2><p>接着我们来讨论，奇异值与特征值分解究竟有什么意义。两者的本质都是矩阵与向量的投影。</p>
<h3 id="向量基的内积与投影"><a href="#向量基的内积与投影" class="headerlink" title="向量基的内积与投影"></a>向量基的内积与投影</h3><p>首先我们来讨论矩阵运算的本质，首先要来讨论向量的内积的意义。
先给一个普通的向量\( (3,4)  \),其中的数字3,4表示的是向量在x轴与y轴上的分量,也就是这个向量在
\( (1, 0) \)与\( (0, 1) \)方向上的<strong>模长</strong>.所以其实向量可以表达成下面这样：
\[
(3,4) = 3 \times (1, 0)^T + 4 \times (0, 1)^T
\]</p>
<p>而\( (1, 0) \)与\( (0, 1) \)就可以作为\( (3,4) \)一个基础。更甚者，所有二维向量其实都可以用
这两个向量通过乘上不用的模来组成，所以这两个向量就是二维向量的一组基。
所以我们也就得到的基向量的定义，<strong>向量空间的基是它的一个特殊的子集，基的元素称为基向量。
向量空间中任意一个元素，都可以唯一地表示成基向量的线性组合。</strong></p>
<p>但是\( (1, 0) \)与\( (0, 1) \) 并不是二维向量空间中唯一的基。
其实我们还可以找到另一组基例如\( (\frac{3}{5}, \frac{4}{5})  \)和\( (\frac{3}{5}, - \frac{4}{5})  \).
而在这组基下，原来的向量\( (3,4) \)就变成了\( (5, 0) \)，因为:
\[
5 \times (\frac{3}{5}, \frac{4}{5})^T + 0 \times (\frac{3}{5}, - \frac{4}{5})^T = (3, 4) = 3 \times (1, 0)^T + 4 \times (0, 1)^T
\]
把上式变的更矩阵化一点，可以得到下式：
\[
\begin{bmatrix}
1 &amp; 0\\
0 &amp; 1
\end{bmatrix} \begin{bmatrix}
3 \\
4
\end{bmatrix} = \begin{bmatrix}
3\\
4
\end{bmatrix} = \begin{bmatrix}
\frac{3}{5} &amp; \frac{3}{5}\\
\frac{4}{5} &amp; -\frac{4}{5}
\end{bmatrix}\begin{bmatrix}
5\\
0
\end{bmatrix}
\]
其中矩阵的每一个行向量都是对应的基向量。
也就是，同一个向量，在不同的基向量下，向量的形式是不同的。而向量中的数字其本质表示的是在对应基向量上的模长。
即:
\[
\vec{v} = [\vec{v}_1, \vec{v}_2, \cdots, \vec{v}_n] [\lambda_1, \lambda_2, \cdots, \lambda_n]^T
\tag{1}
\]
也就说，同一个向量，虽然在不用的基下的向量形式不用，但通过上式计算后，都可以得到同一个结果。
例如上面式(1)所示，向量\( (3,4) \)在两个不同的基下的最终结果就是相等的。</p>
<p>那么如何得到一个向量在不同基下的形式呢？这里就要使用向量的内积。首先是向量内积的定义：
\[
\vec{a} \cdot \vec{b}= \sum\limits_{i = 1}^{n} a_ib_i = a_1b_1 + a_2b_2 + \cdots + a_nb_b
\]
然后我们把内积转化为另一种空间上的形式：
\[
\vec{a} \cdot \vec{b} = |\vec{a}| |\vec{b}| cos(a)
\]
接着再进一步，假设\( |\vec{b}| = 1 \),也就得到了:
\[
\vec{a} \cdot \vec{b} = |\vec{a}| cos(a)
\]
因此可以看出，当基向量的模长为1时，向量与基向量的内积可以理解为了向量的在基向量上投影的模长。
也就如下图：</p>
<p><img src="https://i.ibb.co/5LFKJTk/36936572-25.png" alt="盗个图"></p>
<p>因此通过向量的内积运算，我们就可以很容易的得到向量在基向量上的投影模长。
即通过向量的内积运算，就可以很容易的将向量在不同基向量上的进行转换。
例如之前的向量\( (3,4) \), 在基向量\( (\frac{3}{5}, \frac{4}{5})  \)和
\( (\frac{3}{5}, - \frac{4}{5})  \)
下的投影也就是表现值就是：
\[
(3,4)^T \cdot (\frac{3}{5}, \frac{4}{5})^T = 5 \\
(3,4)^T \cdot (\frac{3}{5}, \frac{4}{5})^T = 0
\]
即：
\[
\begin{bmatrix}
\frac{3}{5} &amp; \frac{3}{5}\\
\frac{4}{5} &amp; -\frac{4}{5}
\end{bmatrix}^T \begin{bmatrix}
3\\ 
4
\end{bmatrix} = \begin{bmatrix}
5\\ 
0
\end{bmatrix}
\]
也就得到了\((5,0) \)这个新的向量。
所以可以看到<strong>向量是可以在不同的基下转换的,而且在不同的基下，向量的形式不一样</strong>.
而将向量转换会标准形式只要需要左乘上这组基向量即可。如下:
\[
\begin{bmatrix}
\frac{3}{5} &amp; \frac{4}{5} \\
\frac{4}{5} &amp; -\frac{3}{5}
\end{bmatrix} \begin{bmatrix}
\frac{3}{5} &amp; \frac{4}{5} \\
\frac{4}{5} &amp; -\frac{3}{5}
\end{bmatrix}^T \begin{bmatrix}
3\\
4
\end{bmatrix} = \begin{bmatrix}
\frac{3}{5} &amp; \frac{4}{5} \\
\frac{4}{5} &amp; -\frac{3}{5}
\end{bmatrix} \begin{bmatrix}
5\\
0
\end{bmatrix} = \begin{bmatrix}
3\\
4
\end{bmatrix}
\]
至此，我知道，向量的内积运算，其本质其实就是<strong>向量的投影。</strong></p>
<h3 id="奇异值分解与矩阵投影"><a href="#奇异值分解与矩阵投影" class="headerlink" title="奇异值分解与矩阵投影"></a>奇异值分解与矩阵投影</h3><p>再理解了内积的本质是投影之后，我们回到矩阵来。
矩阵的本质其实是用来描述线性变换的。
举一个实际的例子.
假设现在有一个点\(p\) 和一个矩阵 \( A \) (暂时不要吐槽这个矩阵为什么这么奇怪，之后就会明白)。如下：
\[
p = \begin{bmatrix}
3\\
4
\end{bmatrix} \quad A = \begin{bmatrix}
3.64 &amp; -0.48 \\
-0.48 &amp; 3.36
\end{bmatrix}
\]
强调一下，这里的\( p \)既可以指一个点，同样也可以表示为一个从原点到该点的向量。
接着我把这个点和矩阵运算一下：
\[
 \begin{bmatrix}
3.64 &amp; -0.48 \\
-0.48 &amp; 3.36
\end{bmatrix}\begin{bmatrix}
3\\
4
\end{bmatrix} = \begin{bmatrix}
9\\
12
\end{bmatrix}
\]
可以看到，矩阵\( A \)就是一个把点\( (3,4) \)转换到了\( (9,12)  \)的线性变换。
接着，我们来做一个有意思的事情，把矩阵A进行特征值分解，结果如下：
\[
A = \begin{bmatrix}
0.6 &amp; 0.8\\
-0.8 &amp; 0.6
\end{bmatrix}\begin{bmatrix}
4 &amp; 0\\
0 &amp; 3
\end{bmatrix}\begin{bmatrix}
0.6 &amp; 0.8\\
-0.8 &amp; 0.6
\end{bmatrix}^T
\]</p>
<p>特征值，奇异值分解的定义：
\[
A \vec{v} = \lambda \vec{v}
\]
这个矩阵内积运算后，依然是这个向量的方向。
也就是说矩阵A，在向量\( \vec{V} \)只产生拉升变换，而不产生旋转变换。也就是如下图：</p>
<p><img src="https://image.ibb.co/ki2JY8/exp3.png" alt="特征向量和特征空间"></p>
<p>似乎是有点抽象，我们来</p>
<p>接着，我们把矩阵\( A \)进行特征值分解可以得到:</p>
<p>再接着，我们把向量\( (1,2) \) 投影到新的基向量上。</p>
<h1 id="奇异值分解的应用"><a href="#奇异值分解的应用" class="headerlink" title="奇异值分解的应用"></a>奇异值分解的应用</h1><h2 id="奇异分解与主成分分析-PCA"><a href="#奇异分解与主成分分析-PCA" class="headerlink" title="奇异分解与主成分分析(PCA)"></a>奇异分解与主成分分析(PCA)</h2><h2 id="奇异值分解与最小二乘"><a href="#奇异值分解与最小二乘" class="headerlink" title="奇异值分解与最小二乘"></a>奇异值分解与最小二乘</h2><h3 id="最小二乘法与数据拟合"><a href="#最小二乘法与数据拟合" class="headerlink" title="最小二乘法与数据拟合"></a>最小二乘法与数据拟合</h3><h3 id="最小二乘法与多项式拟合"><a href="#最小二乘法与多项式拟合" class="headerlink" title="最小二乘法与多项式拟合"></a>最小二乘法与多项式拟合</h3><h1 id="向量的基，内积与向量投影"><a href="#向量的基，内积与向量投影" class="headerlink" title="向量的基，内积与向量投影"></a>向量的基，内积与向量投影</h1><h1 id="特征值与特征向量"><a href="#特征值与特征向量" class="headerlink" title="特征值与特征向量"></a>特征值与特征向量</h1><p>之前我们展示了向量是可以投影到另一个向量上的。
那么扩展一下，矩阵是不是也可以投影到一个向量上呢？
答案是肯定的，特征值和特征向量就可以办到。</p>
<p>首先看一下特征向量的定义：
\[
A\vec{V} = \lambda\vec{V} \\
A = P \Lambda P^{-1} 
\]
上式的几何意义就是</p>

      
    </div>
    <footer>
      
        
        
  
  <div class="tags">
    <a href="/tags/图像处理/">图像处理</a>
  </div>

        
  <div class="addthis addthis_toolbox addthis_default_style">
    
      <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
    
    
      <a class="addthis_button_tweet"></a>
    
    
      <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    
    
      <a class="addthis_button_pinterest_pinit" pi:pinit:layout="horizontal"></a>
    
    <a class="addthis_counter addthis_pill_style"></a>
  </div>
  <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js"></script>

      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>


<section id="comment">
  <h1 class="title">Comments</h1>

  
      <div id="fb-root"></div>
<script>
  (function(d, s, id) {
    var js, fjs = d.getElementsByTagName(s)[0];
    if (d.getElementById(id)) return;
    js = d.createElement(s); js.id = id;
    js.src = "//connect.facebook.net/en_US/all.js#xfbml=1&appId=378375629598606";
    fjs.parentNode.insertBefore(js, fjs);
  }(document, 'script', 'facebook-jssdk'));
</script>

<div class="fb-comments" data-href="http://yoursite.com/2018/12/06/eigenvalue/index.html" data-num-posts="5" data-width="840" data-colorscheme="light"></div>

      
  
</section>

</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Search">
    <input type="hidden" name="q" value="site:yoursite.com">
  </form>
</div>

  

  
<div class="widget tag">
  <h3 class="title">Tags</h3>
  <ul class="entry">
  
    <li><a href="/tags/C-数据结构/">C++/数据结构</a><small>1</small></li>
  
    <li><a href="/tags/Linux/">Linux</a><small>4</small></li>
  
    <li><a href="/tags/Lua/">Lua</a><small>1</small></li>
  
    <li><a href="/tags/Pandoc/">Pandoc</a><small>1</small></li>
  
    <li><a href="/tags/RouterOs/">RouterOs</a><small>1</small></li>
  
    <li><a href="/tags/python/">python</a><small>1</small></li>
  
    <li><a href="/tags/图像处理/">图像处理</a><small>7</small></li>
  
    <li><a href="/tags/数学公式/">数学公式</a><small>1</small></li>
  
    <li><a href="/tags/杂文/">杂文</a><small>1</small></li>
  
    <li><a href="/tags/概率与统计/">概率与统计</a><small>1</small></li>
  
    <li><a href="/tags/算法/">算法</a><small>1</small></li>
  
    <li><a href="/tags/计算机网络/">计算机网络</a><small>1</small></li>
  
  </ul>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2019 Zhao, Chenqiu
  
</div>
<div class="clearfix"></div></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>




<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>

</body>
</html>
